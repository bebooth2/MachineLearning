{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What Linear Regression training algorithm can you use if you have a training set with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lasso and Elastic Net help to reduce the importance of useless features, by lowering there weights. But if the number of traning instances is less than the number of features, or several features are stronly correlated then use Elastic Net because it behaves less erratically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Gradient Descent algorithms on MSE cost function if your training data is not scaled you will create a elongated surface. Since you are giving it a learning rate (i.e. steps) to decrease the cost function if the features are not scaled it will take longer to reach its minimum on those features that have larger values and ranges.\n",
    "Also when using the PolynomialFeatures class on your training set after you fit_transform your data you should use the scale method. And last but not least you should scale data for most of your regularized models, like Ridge and lasso. Ridge Regression and others are sensitive to the scale of the input features. StandardScaler class has a fit_transform method to scale you training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function for the Logistic Regression model is convex, so Gradient Descent(or any other optimization algorithm) is guaranteed to find the global minimum, as long as the learning rate is not too large and you wait long enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. Batch Gradient Descent will settle to the global minimum but Stochastic and mini_Batch will bounce around the global minimum. But with the last two you can write a function called the learning schedule that gradually lowers the learning rate so it settles at a minimum. But there are problems with that approach and if you lower you rate to fast you might find yourself in a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice the the validation error consistently goes up, what is likely going on? How can you fix this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your learning rate is too high the algorithm diverges, jumping all over the place and can actually go farther and farther away from the solution. But if the training errors are not going up then you have overfitted the model and should consider reducing the number of features or look at the degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. Like Stochasic Gradient, Mini-batch will jump around but since you are training it on more than one instance per interval you are going to find the minimum faster than the Stochastic method. But remember to save your theta on a regular basis. (Look more into this, should be able to save for a reason not just randomly, possibly checking the cost function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Which Gradient Descent algorithm(among those discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since mini-batch uses a set of training instances it gets a performance boost from hardware optimization of matrix operations, especially when using GPUs. Batch will actually converge to the optimal model but you can create a function for the learning rate that reduces the size of the steps it takes towards the end. But you do risk being caught in a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Suppose you are using Polynomial Regression. You plot the learning curves and notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your training error is much less that your validation error than you are overfitting the model. Since you set the degrees of freedom when you instantiate your PolynomialFeatures object you can reduce the degrees before you call the fit_transform method. Another way to reduce the validation error is to use a larger training set. Also if you are just using a plain linear regression model you could use one of the regularized models like Ridge or Lasso Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter $\\alpha$ or reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that underfits the training data will have a high validation error and training error. A high-bias model is most likey to underfit the training data. The regularizatin hyperparameter $\\alpha$ should be increased. If the model had a high variance that means that it is overly sensitive to the training data's variances or outliers, thus overfitting the model. The model would than want a reduction in its $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Why would you want to use:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression over plain Linear Regression(ie without any regularization)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's almost always preferable to have a least a little bit of regulariztion, so it best to avoid plain Linear Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso instead of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important characteristic of Lasso Regression is that it tends to completely eliminate the least important features by setting there weights to zero. If you feel some of your features are not important than you Lasso instead of Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net has a mix ratio term r that is set to the amount of Lasso Regression you want in your model. If it is set to r =0 , then it is equavalent to Ridge Regression and when it is set to r = 1, then it is Lasso. Elastic Net is usually preferred over Lassom but especially when the number of features is greater than the number of training instances or when several of the features are strongly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would need to implement two Logistic Regression classifiers. Softmax Regression classifer classifies mutually exclusive class. Since you could have an indoor daytime picture or an outdoor daytime picture the classes would not be mutually exclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Implement Batch Gradient Descent with early stopping for Softmax Regression(without using Scikit-Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Decent step\n",
    "\n",
    "$\\theta^{(next step)}$  = $\\theta$ - $n$$\\nabla_{\\theta}$MSE($\\theta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the gradient descent step for a linear models cost function. Minimizing the cost function for Softmax is called cross entropy.\n",
    "\n",
    "J($\\Theta$) = -$\\frac{1}{m}$$\\sum_{i=1}^{m}$$\\sum_{k=1}^{K}$$y_{k}^{(i)}$log($\\hat{p}_{k}^{(i)}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient vector of this cost function with respect to $\\theta^{(k)}$ is\n",
    "\n",
    "$\\nabla_{\\theta}{k}$ J($\\Theta$) = $\\frac{1}{m}$$\\sum_{i=1}^{m}$($\\hat{p}_{k}^{(i)}$ - $y_{k}^{(i)}$)x$^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = iris[\"data\"][:, (2,3)]\n",
    "target_values = iris[\"target\"]\n",
    "target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column with all values equal to 1 for our bias theta\n",
    "import numpy as np\n",
    "X_data = np.c_[np.ones([len(data),1]), data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a seed for reproducible results\n",
    "np.random.seed(24)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first creating a test set\n",
    "# note that my X_val will not be the 20 of the whole training set like above\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, target_values, test_size = .20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = .20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target_values were an array of 0,1,2 s. But for classification we need each of those values in their own column since they disinctly identify a class and do not represent numerical value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(y):\n",
    "    m = len(y)\n",
    "    n_classes = y.max() +1\n",
    "    #creating a m by 3 array of zeros\n",
    "    target = np.zeros((m,n_classes))\n",
    "    # for each row of the target set the column(the y value) equal to one\n",
    "    #numpy.arange([start, ]stop, [step, ]dtype=None)\n",
    "    target[np.arange(m), y] = 1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert our target values\n",
    "y_test_one_hot = oneHot(y_test)\n",
    "y_train_one_hot = oneHot(y_train)\n",
    "y_val_one_hot = oneHot(y_val)\n",
    "y_val_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Softmax function. remeber $s_{k}(x))$ is a different function so lets first define a function\n",
    "to deal just with the outside part\n",
    "\n",
    "$\\hat{p}_k$ = $\\sigma$(s(x))$_{k}$ = $\\frac{exp(s_{k}(x))}{\\sum_{j=1}^{K} exp(s_{j}(x))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we pass in, is our Softmax score for class k\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    sum_exps = np.sum(exps, axis =1, keepdims = True)\n",
    "    return exps/sum_exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]# gives us the number of theta terms\n",
    "                           # one for the bias term and one for width and length\n",
    "n_outputs = len(np.unique(y_train)) # gives us the number of classes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs\n",
    "n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.2837418339674245\n",
      "500 0.8641262387549173\n",
      "1000 0.7080205759817026\n",
      "1500 0.6160746040125203\n",
      "2000 0.5565347688837092\n",
      "2500 0.5145050284731898\n",
      "3000 0.4827841976929624\n",
      "3500 0.4576199616158299\n",
      "4000 0.4369013316456087\n",
      "4500 0.41935763552680133\n",
      "5000 0.4041788923815168\n"
     ]
    }
   ],
   "source": [
    "# to train our model\n",
    "eta = .01 # our learning rate\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7 # added for when our projected probability is 0. if we add this time number we wont get NaN values\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)# creating a matrix theta instead of a array because instead \n",
    "                                            # of just one class we have multiple. each class gets its own\n",
    "                                            # array of theta weights\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # okay I would have Theta tranposed then dot X_train ??\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    # this is the Cross entropy cost function\n",
    "    # the Y_train_one_hot is the expected and the Y_proba is the calculated\n",
    "    # remember we add epsilon just to stop a NaN for log of zero\n",
    "    loss = -np.mean(np.sum(y_train_one_hot * np.log(Y_proba +epsilon), axis = 1))\n",
    "    error = Y_proba - y_train_one_hot\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)\n",
    "    # this is the Cross entropy gradient vector for class k\n",
    "    # again this is backwards from what you would have done\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta *gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.13214623,  0.44771909, -1.65883437],\n",
       "       [-0.38694137,  0.7041692 ,  0.44634073],\n",
       "       [-2.51712208, -1.32616406,  0.84978724]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that is it the soft max model is train lets look at the matrix for model parameters of each class\n",
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_test.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis = 1)\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
